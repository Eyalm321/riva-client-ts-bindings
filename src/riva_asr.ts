// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.0.2
//   protoc               v5.28.0
// source: riva/proto/riva_asr.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import {
  type CallOptions,
  ChannelCredentials,
  Client,
  ClientDuplexStream,
  type ClientOptions,
  type ClientUnaryCall,
  handleBidiStreamingCall,
  type handleUnaryCall,
  makeGenericClientConstructor,
  Metadata,
  type ServiceError,
  type UntypedServiceImplementation,
} from "@grpc/grpc-js";
import { AudioEncoding, audioEncodingFromJSON, audioEncodingToJSON } from "./riva_audio";
import { RequestId } from "./riva_common";

export const protobufPackage = "nvidia.riva.asr";

export interface RivaSpeechRecognitionConfigRequest {
  /**
   * If model is specified only return config for model, otherwise return all
   * configs.
   */
  modelName: string;
}

export interface RivaSpeechRecognitionConfigResponse {
  modelConfig: RivaSpeechRecognitionConfigResponse_Config[];
}

export interface RivaSpeechRecognitionConfigResponse_Config {
  modelName: string;
  parameters: { [key: string]: string };
}

export interface RivaSpeechRecognitionConfigResponse_Config_ParametersEntry {
  key: string;
  value: string;
}

/** RecognizeRequest is used for batch processing of a single audio recording. */
export interface RecognizeRequest {
  /**
   * Provides information to recognizer that specifies how to process the
   * request.
   */
  config:
    | RecognitionConfig
    | undefined;
  /**
   * The raw audio data to be processed. The audio bytes must be encoded as
   * specified in `RecognitionConfig`.
   */
  audio: Uint8Array;
  /**
   * The ID to be associated with the request. If provided, this will be
   * returned in the corresponding response.
   */
  id: RequestId | undefined;
}

/**
 * A StreamingRecognizeRequest is used to configure and stream audio content to
 * the Riva ASR Service. The first message sent must include only a
 * StreamingRecognitionConfig. Subsequent messages sent in the stream must
 * contain only raw bytes of the audio to be recognized.
 */
export interface StreamingRecognizeRequest {
  /**
   * Provides information to the recognizer that specifies how to process the
   * request. The first `StreamingRecognizeRequest` message must contain a
   * `streaming_config`  message.
   */
  streamingConfig?:
    | StreamingRecognitionConfig
    | undefined;
  /**
   * The audio data to be recognized. Sequential chunks of audio data are sent
   * in sequential `StreamingRecognizeRequest` messages. The first
   * `StreamingRecognizeRequest` message must not contain `audio` data
   * and all subsequent `StreamingRecognizeRequest` messages must contain
   * `audio` data. The audio bytes must be encoded as specified in
   * `RecognitionConfig`.
   */
  audioContent?:
    | Uint8Array
    | undefined;
  /**
   * The ID to be associated with the request. If provided, this will be
   * returned in the corresponding responses.
   */
  id: RequestId | undefined;
}

/**
 * EndpointingConfig is used for configuring different fields related to start
 * or end of utterance
 */
export interface EndpointingConfig {
  /**
   * `start_history` is the size of the window, in milliseconds, used to
   * detect start of utterance.
   * `start_threshold` is the percentage threshold used to detect start of
   * utterance. (0.0 to 1.0)
   * If `start_threshold` of `start_history` ms of the acoustic model output
   * have non-blank tokens, start of utterance is detected.
   */
  startHistory?: number | undefined;
  startThreshold?:
    | number
    | undefined;
  /**
   * `stop_history` is the size of the window, in milliseconds, used to
   * detect end of utterance.
   * `stop_threshold` is the percentage threshold used to detect end of
   * utterance. (0.0 to 1.0)
   * If `stop_threshold` of `stop_history` ms of the acoustic model output have
   * non-blank tokens, end of utterance is detected and decoder will be reset.
   */
  stopHistory?: number | undefined;
  stopThreshold?:
    | number
    | undefined;
  /**
   * `stop_history_eou` and `stop_threshold_eou` are used for 2-pass end of utterance.
   * `stop_history_eou` is the size of the window, in milliseconds, used to
   * trigger 1st pass of end of utterance and generate a partial transcript
   * with stability of 1. (stop_history_eou < stop_history)
   * `stop_threshold_eou` is the percentage threshold used to trigger 1st
   * pass of end of utterance. (0.0 to 1.0)
   * If `stop_threshold_eou` of `stop_history_eou` ms of the acoustic model
   * output have non-blank tokens, 1st pass of end of utterance is triggered.
   */
  stopHistoryEou?: number | undefined;
  stopThresholdEou?: number | undefined;
}

/**
 * Provides information to the recognizer that specifies how to process the
 * request
 */
export interface RecognitionConfig {
  /**
   * The encoding of the audio data sent in the request.
   *
   * All encodings support only 1 channel (mono) audio.
   */
  encoding: AudioEncoding;
  /**
   * The sample rate in hertz (Hz) of the audio data sent in the
   * `RecognizeRequest` or `StreamingRecognizeRequest` messages.
   *  The Riva server will automatically down-sample/up-sample the audio to
   *  match the ASR acoustic model sample rate. The sample rate value below 8kHz
   *  will not produce any meaningful output.
   */
  sampleRateHertz: number;
  /**
   * Required. The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   */
  languageCode: string;
  /**
   * Maximum number of recognition hypotheses to be returned.
   * Specifically, the maximum number of `SpeechRecognizeAlternative` messages
   * within each `SpeechRecognizeResult`.
   * The server may return fewer than `max_alternatives`.
   * If omitted, will return a maximum of one.
   */
  maxAlternatives: number;
  /**
   * A custom field that enables profanity filtering for the generated
   * transcripts. If set to 'true', the server filters out profanities,
   * replacing all but the initial character in each filtered word with
   * asterisks. For example, "x**". If set to `false` or omitted, profanities
   * will not be filtered out. The default is `false`.
   */
  profanityFilter: boolean;
  /**
   * Array of SpeechContext.
   * A means to provide context to assist the speech recognition. For more
   * information, see SpeechContext section
   */
  speechContexts: SpeechContext[];
  /**
   * The number of channels in the input audio data.
   * If `0` or omitted, defaults to one channel (mono).
   * Note: Only single channel audio input is supported as of now.
   */
  audioChannelCount: number;
  /**
   * If `true`, the top result includes a list of words and the start and end
   * time offsets (timestamps), and confidence scores for those words. If
   * `false`, no word-level time offset information is returned. The default
   * is `false`.
   */
  enableWordTimeOffsets: boolean;
  /**
   * If 'true', adds punctuation to recognition result hypotheses. The
   * default 'false' value does not add punctuation to result hypotheses.
   */
  enableAutomaticPunctuation: boolean;
  /**
   * This needs to be set to `true` explicitly and `audio_channel_count` > 1
   * to get each channel recognized separately. The recognition result will
   * contain a `channel_tag` field to state which channel that result belongs
   * to. If this is not true, we will only recognize the first channel. The
   * request is billed cumulatively for all channels recognized:
   * `audio_channel_count` multiplied by the length of the audio.
   * Note: This field is not yet supported.
   */
  enableSeparateRecognitionPerChannel: boolean;
  /**
   * Which model to select for the given request.
   * If empty, Riva will select the right model based on the other
   * RecognitionConfig parameters. The model should correspond to the name
   * passed to `riva-build` with the `--name` argument
   */
  model: string;
  /**
   * The verbatim_transcripts flag enables or disable inverse text
   * normalization. 'true' returns exactly what was said, with no
   * denormalization. 'false' applies inverse text normalization, also this is
   * the default
   */
  verbatimTranscripts: boolean;
  /**
   * Config to enable speaker diarization and set additional
   * parameters. For non-streaming requests, the diarization results will be
   * provided only in the top alternative of the FINAL SpeechRecognitionResult.
   */
  diarizationConfig:
    | SpeakerDiarizationConfig
    | undefined;
  /**
   * Custom fields for passing request-level
   * configuration options to plugins used in the
   * model pipeline.
   */
  customConfiguration: { [key: string]: string };
  /**
   * Config for tuning start or end of utterance parameters.
   * If empty, Riva will use default values or custom values if specified in riva-build arguments.
   */
  endpointingConfig?: EndpointingConfig | undefined;
}

export interface RecognitionConfig_CustomConfigurationEntry {
  key: string;
  value: string;
}

/**
 * Provides information to the recognizer that specifies how to process the
 * request
 */
export interface StreamingRecognitionConfig {
  /**
   * Provides information to the recognizer that specifies how to process the
   * request
   */
  config:
    | RecognitionConfig
    | undefined;
  /**
   * If `true`, interim results (tentative hypotheses) may be
   * returned as they become available (these interim results are indicated with
   * the `is_final=false` flag).
   * If `false` or omitted, only `is_final=true` result(s) are returned.
   */
  interimResults: boolean;
}

/** Config to enable speaker diarization. */
export interface SpeakerDiarizationConfig {
  /**
   * If 'true', enables speaker detection for each recognized word in
   * the top alternative of the recognition result using a speaker_tag provided
   * in the WordInfo.
   */
  enableSpeakerDiarization: boolean;
  /**
   * Maximum number of speakers in the conversation. This gives flexibility by
   * allowing the system to automatically determine the correct number of
   * speakers. If not set, the default value is 8.
   */
  maxSpeakerCount: number;
}

/**
 * Provides "hints" to the speech recognizer to favor specific words and phrases
 * in the results.
 */
export interface SpeechContext {
  /**
   * A list of strings containing words and phrases "hints" so that
   * the speech recognition is more likely to recognize them. This can be used
   * to improve the accuracy for specific words and phrases, for example, if
   * specific commands are typically spoken by the user. This can also be used
   * to add additional words to the vocabulary of the recognizer.
   */
  phrases: string[];
  /**
   * Hint Boost. Positive value will increase the probability that a specific
   * phrase will be recognized over other similar sounding phrases. The higher
   * the boost, the higher the chance of false positive recognition as well.
   * Though `boost` can accept a wide range of positive values, most use cases
   * are best served with values between 0 and 20. We recommend using a binary
   * search approach to finding the optimal value for your use case.
   */
  boost: number;
}

/**
 * The only message returned to the client by the `Recognize` method. It
 * contains the result as zero or more sequential `SpeechRecognitionResult`
 * messages.
 */
export interface RecognizeResponse {
  /**
   * Sequential list of transcription results corresponding to
   * sequential portions of audio. Currently only returns one transcript.
   */
  results: SpeechRecognitionResult[];
  /** The ID associated with the request */
  id: RequestId | undefined;
}

/** A speech recognition result corresponding to the latest transcript */
export interface SpeechRecognitionResult {
  /**
   * May contain one or more recognition hypotheses (up to the
   * maximum specified in `max_alternatives`).
   * These alternatives are ordered in terms of accuracy, with the top (first)
   * alternative being the most probable, as ranked by the recognizer.
   */
  alternatives: SpeechRecognitionAlternative[];
  /**
   * For multi-channel audio, this is the channel number corresponding to the
   * recognized result for the audio from that channel.
   * For audio_channel_count = N, its output values can range from '1' to 'N'.
   */
  channelTag: number;
  /** Length of audio processed so far in seconds */
  audioProcessed: number;
}

/** Alternative hypotheses (a.k.a. n-best list). */
export interface SpeechRecognitionAlternative {
  /** Transcript text representing the words that the user spoke. */
  transcript: string;
  /**
   * The confidence estimate. A higher number indicates an estimated greater
   * likelihood that the recognized word is correct. This field is set only for
   * a non-streaming result or, for a streaming result where is_final=true.
   * This field is not guaranteed to be accurate and users should not rely on
   * it to be always provided. Although confidence can currently be roughly
   * interpreted as a natural-log probability, the estimate computation varies
   * with difference configurations, and is subject to change. The default of
   * 0.0 is a sentinel value indicating confidence was not set.
   */
  confidence: number;
  /**
   * A list of word-specific information for each recognized word. Only
   * populated if is_final=true
   */
  words: WordInfo[];
}

/** Word-specific information for recognized words. */
export interface WordInfo {
  /**
   * Time offset relative to the beginning of the audio in ms
   * and corresponding to the start of the spoken word.
   * This field is only set if `enable_word_time_offsets=true` and only
   * in the top hypothesis.
   */
  startTime: number;
  /**
   * Time offset relative to the beginning of the audio in ms
   * and corresponding to the end of the spoken word.
   * This field is only set if `enable_word_time_offsets=true` and only
   * in the top hypothesis.
   */
  endTime: number;
  /** The word corresponding to this set of information. */
  word: string;
  /**
   * The confidence estimate. A higher number indicates an estimated greater
   * likelihood that the recognized word is correct. This field is set only for
   * a non-streaming result or, for a streaming result where is_final=true.
   * This field is not guaranteed to be accurate and users should not rely on
   * it to be always provided. Although confidence can currently be roughly
   * interpreted as a natural-log probability, the estimate computation varies
   * with difference configurations, and is subject to change. The default of
   * 0.0 is a sentinel value indicating confidence was not set.
   */
  confidence: number;
  /**
   * Output only. A distinct integer value is assigned for every speaker within
   * the audio. This field specifies which one of those speakers was detected to
   * have spoken this word. Value ranges from '1' to diarization_speaker_count.
   * speaker_tag is set if enable_speaker_diarization = 'true' and only in the
   * top alternative.
   */
  speakerTag: number;
}

export interface StreamingRecognizeResponse {
  /**
   * This repeated list contains the latest transcript(s) corresponding to
   * audio currently being processed.
   * Currently one result is returned, where each result can have multiple
   * alternatives
   */
  results: StreamingRecognitionResult[];
  /** The ID associated with the request */
  id: RequestId | undefined;
}

/**
 * A streaming speech recognition result corresponding to a portion of the audio
 * that is currently being processed.
 */
export interface StreamingRecognitionResult {
  /**
   * May contain one or more recognition hypotheses (up to the
   * maximum specified in `max_alternatives`).
   * These alternatives are ordered in terms of accuracy, with the top (first)
   * alternative being the most probable, as ranked by the recognizer.
   */
  alternatives: SpeechRecognitionAlternative[];
  /**
   * If `false`, this `StreamingRecognitionResult` represents an
   * interim result that may change. If `true`, this is the final time the
   * speech service will return this particular `StreamingRecognitionResult`,
   * the recognizer will not return any further hypotheses for this portion of
   * the transcript and corresponding audio.
   */
  isFinal: boolean;
  /**
   * An estimate of the likelihood that the recognizer will not
   * change its guess about this interim result. Values range from 0.0
   * (completely unstable) to 1.0 (completely stable).
   * This field is only provided for interim results (`is_final=false`).
   * The default of 0.0 is a sentinel value indicating `stability` was not set.
   */
  stability: number;
  /**
   * For multi-channel audio, this is the channel number corresponding to the
   * recognized result for the audio from that channel.
   * For audio_channel_count = N, its output values can range from '1' to 'N'.
   */
  channelTag: number;
  /** Length of audio processed so far in seconds */
  audioProcessed: number;
}

function createBaseRivaSpeechRecognitionConfigRequest(): RivaSpeechRecognitionConfigRequest {
  return { modelName: "" };
}

export const RivaSpeechRecognitionConfigRequest = {
  encode(message: RivaSpeechRecognitionConfigRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.modelName !== "") {
      writer.uint32(10).string(message.modelName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RivaSpeechRecognitionConfigRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRivaSpeechRecognitionConfigRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.modelName = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RivaSpeechRecognitionConfigRequest {
    return { modelName: isSet(object.modelName) ? globalThis.String(object.modelName) : "" };
  },

  toJSON(message: RivaSpeechRecognitionConfigRequest): unknown {
    const obj: any = {};
    if (message.modelName !== "") {
      obj.modelName = message.modelName;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<RivaSpeechRecognitionConfigRequest>, I>>(
    base?: I,
  ): RivaSpeechRecognitionConfigRequest {
    return RivaSpeechRecognitionConfigRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<RivaSpeechRecognitionConfigRequest>, I>>(
    object: I,
  ): RivaSpeechRecognitionConfigRequest {
    const message = createBaseRivaSpeechRecognitionConfigRequest();
    message.modelName = object.modelName ?? "";
    return message;
  },
};

function createBaseRivaSpeechRecognitionConfigResponse(): RivaSpeechRecognitionConfigResponse {
  return { modelConfig: [] };
}

export const RivaSpeechRecognitionConfigResponse = {
  encode(message: RivaSpeechRecognitionConfigResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.modelConfig) {
      RivaSpeechRecognitionConfigResponse_Config.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RivaSpeechRecognitionConfigResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRivaSpeechRecognitionConfigResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.modelConfig.push(RivaSpeechRecognitionConfigResponse_Config.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RivaSpeechRecognitionConfigResponse {
    return {
      modelConfig: globalThis.Array.isArray(object?.modelConfig)
        ? object.modelConfig.map((e: any) => RivaSpeechRecognitionConfigResponse_Config.fromJSON(e))
        : [],
    };
  },

  toJSON(message: RivaSpeechRecognitionConfigResponse): unknown {
    const obj: any = {};
    if (message.modelConfig?.length) {
      obj.modelConfig = message.modelConfig.map((e) => RivaSpeechRecognitionConfigResponse_Config.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<RivaSpeechRecognitionConfigResponse>, I>>(
    base?: I,
  ): RivaSpeechRecognitionConfigResponse {
    return RivaSpeechRecognitionConfigResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<RivaSpeechRecognitionConfigResponse>, I>>(
    object: I,
  ): RivaSpeechRecognitionConfigResponse {
    const message = createBaseRivaSpeechRecognitionConfigResponse();
    message.modelConfig = object.modelConfig?.map((e) => RivaSpeechRecognitionConfigResponse_Config.fromPartial(e)) ||
      [];
    return message;
  },
};

function createBaseRivaSpeechRecognitionConfigResponse_Config(): RivaSpeechRecognitionConfigResponse_Config {
  return { modelName: "", parameters: {} };
}

export const RivaSpeechRecognitionConfigResponse_Config = {
  encode(message: RivaSpeechRecognitionConfigResponse_Config, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.modelName !== "") {
      writer.uint32(10).string(message.modelName);
    }
    Object.entries(message.parameters).forEach(([key, value]) => {
      RivaSpeechRecognitionConfigResponse_Config_ParametersEntry.encode(
        { key: key as any, value },
        writer.uint32(18).fork(),
      ).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RivaSpeechRecognitionConfigResponse_Config {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRivaSpeechRecognitionConfigResponse_Config();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.modelName = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          const entry2 = RivaSpeechRecognitionConfigResponse_Config_ParametersEntry.decode(reader, reader.uint32());
          if (entry2.value !== undefined) {
            message.parameters[entry2.key] = entry2.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RivaSpeechRecognitionConfigResponse_Config {
    return {
      modelName: isSet(object.modelName) ? globalThis.String(object.modelName) : "",
      parameters: isObject(object.parameters)
        ? Object.entries(object.parameters).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: RivaSpeechRecognitionConfigResponse_Config): unknown {
    const obj: any = {};
    if (message.modelName !== "") {
      obj.modelName = message.modelName;
    }
    if (message.parameters) {
      const entries = Object.entries(message.parameters);
      if (entries.length > 0) {
        obj.parameters = {};
        entries.forEach(([k, v]) => {
          obj.parameters[k] = v;
        });
      }
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<RivaSpeechRecognitionConfigResponse_Config>, I>>(
    base?: I,
  ): RivaSpeechRecognitionConfigResponse_Config {
    return RivaSpeechRecognitionConfigResponse_Config.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<RivaSpeechRecognitionConfigResponse_Config>, I>>(
    object: I,
  ): RivaSpeechRecognitionConfigResponse_Config {
    const message = createBaseRivaSpeechRecognitionConfigResponse_Config();
    message.modelName = object.modelName ?? "";
    message.parameters = Object.entries(object.parameters ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseRivaSpeechRecognitionConfigResponse_Config_ParametersEntry(): RivaSpeechRecognitionConfigResponse_Config_ParametersEntry {
  return { key: "", value: "" };
}

export const RivaSpeechRecognitionConfigResponse_Config_ParametersEntry = {
  encode(
    message: RivaSpeechRecognitionConfigResponse_Config_ParametersEntry,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): RivaSpeechRecognitionConfigResponse_Config_ParametersEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRivaSpeechRecognitionConfigResponse_Config_ParametersEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RivaSpeechRecognitionConfigResponse_Config_ParametersEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: RivaSpeechRecognitionConfigResponse_Config_ParametersEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<RivaSpeechRecognitionConfigResponse_Config_ParametersEntry>, I>>(
    base?: I,
  ): RivaSpeechRecognitionConfigResponse_Config_ParametersEntry {
    return RivaSpeechRecognitionConfigResponse_Config_ParametersEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<RivaSpeechRecognitionConfigResponse_Config_ParametersEntry>, I>>(
    object: I,
  ): RivaSpeechRecognitionConfigResponse_Config_ParametersEntry {
    const message = createBaseRivaSpeechRecognitionConfigResponse_Config_ParametersEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseRecognizeRequest(): RecognizeRequest {
  return { config: undefined, audio: new Uint8Array(0), id: undefined };
}

export const RecognizeRequest = {
  encode(message: RecognizeRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.config !== undefined) {
      RecognitionConfig.encode(message.config, writer.uint32(10).fork()).join();
    }
    if (message.audio.length !== 0) {
      writer.uint32(18).bytes(message.audio);
    }
    if (message.id !== undefined) {
      RequestId.encode(message.id, writer.uint32(802).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RecognizeRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRecognizeRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.config = RecognitionConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.audio = reader.bytes();
          continue;
        case 100:
          if (tag !== 802) {
            break;
          }

          message.id = RequestId.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RecognizeRequest {
    return {
      config: isSet(object.config) ? RecognitionConfig.fromJSON(object.config) : undefined,
      audio: isSet(object.audio) ? bytesFromBase64(object.audio) : new Uint8Array(0),
      id: isSet(object.id) ? RequestId.fromJSON(object.id) : undefined,
    };
  },

  toJSON(message: RecognizeRequest): unknown {
    const obj: any = {};
    if (message.config !== undefined) {
      obj.config = RecognitionConfig.toJSON(message.config);
    }
    if (message.audio.length !== 0) {
      obj.audio = base64FromBytes(message.audio);
    }
    if (message.id !== undefined) {
      obj.id = RequestId.toJSON(message.id);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<RecognizeRequest>, I>>(base?: I): RecognizeRequest {
    return RecognizeRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<RecognizeRequest>, I>>(object: I): RecognizeRequest {
    const message = createBaseRecognizeRequest();
    message.config = (object.config !== undefined && object.config !== null)
      ? RecognitionConfig.fromPartial(object.config)
      : undefined;
    message.audio = object.audio ?? new Uint8Array(0);
    message.id = (object.id !== undefined && object.id !== null) ? RequestId.fromPartial(object.id) : undefined;
    return message;
  },
};

function createBaseStreamingRecognizeRequest(): StreamingRecognizeRequest {
  return { streamingConfig: undefined, audioContent: undefined, id: undefined };
}

export const StreamingRecognizeRequest = {
  encode(message: StreamingRecognizeRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.streamingConfig !== undefined) {
      StreamingRecognitionConfig.encode(message.streamingConfig, writer.uint32(10).fork()).join();
    }
    if (message.audioContent !== undefined) {
      writer.uint32(18).bytes(message.audioContent);
    }
    if (message.id !== undefined) {
      RequestId.encode(message.id, writer.uint32(802).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingRecognizeRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingRecognizeRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.streamingConfig = StreamingRecognitionConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.audioContent = reader.bytes();
          continue;
        case 100:
          if (tag !== 802) {
            break;
          }

          message.id = RequestId.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingRecognizeRequest {
    return {
      streamingConfig: isSet(object.streamingConfig)
        ? StreamingRecognitionConfig.fromJSON(object.streamingConfig)
        : undefined,
      audioContent: isSet(object.audioContent) ? bytesFromBase64(object.audioContent) : undefined,
      id: isSet(object.id) ? RequestId.fromJSON(object.id) : undefined,
    };
  },

  toJSON(message: StreamingRecognizeRequest): unknown {
    const obj: any = {};
    if (message.streamingConfig !== undefined) {
      obj.streamingConfig = StreamingRecognitionConfig.toJSON(message.streamingConfig);
    }
    if (message.audioContent !== undefined) {
      obj.audioContent = base64FromBytes(message.audioContent);
    }
    if (message.id !== undefined) {
      obj.id = RequestId.toJSON(message.id);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<StreamingRecognizeRequest>, I>>(base?: I): StreamingRecognizeRequest {
    return StreamingRecognizeRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<StreamingRecognizeRequest>, I>>(object: I): StreamingRecognizeRequest {
    const message = createBaseStreamingRecognizeRequest();
    message.streamingConfig = (object.streamingConfig !== undefined && object.streamingConfig !== null)
      ? StreamingRecognitionConfig.fromPartial(object.streamingConfig)
      : undefined;
    message.audioContent = object.audioContent ?? undefined;
    message.id = (object.id !== undefined && object.id !== null) ? RequestId.fromPartial(object.id) : undefined;
    return message;
  },
};

function createBaseEndpointingConfig(): EndpointingConfig {
  return {
    startHistory: undefined,
    startThreshold: undefined,
    stopHistory: undefined,
    stopThreshold: undefined,
    stopHistoryEou: undefined,
    stopThresholdEou: undefined,
  };
}

export const EndpointingConfig = {
  encode(message: EndpointingConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startHistory !== undefined) {
      writer.uint32(8).int32(message.startHistory);
    }
    if (message.startThreshold !== undefined) {
      writer.uint32(21).float(message.startThreshold);
    }
    if (message.stopHistory !== undefined) {
      writer.uint32(24).int32(message.stopHistory);
    }
    if (message.stopThreshold !== undefined) {
      writer.uint32(37).float(message.stopThreshold);
    }
    if (message.stopHistoryEou !== undefined) {
      writer.uint32(40).int32(message.stopHistoryEou);
    }
    if (message.stopThresholdEou !== undefined) {
      writer.uint32(53).float(message.stopThresholdEou);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EndpointingConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEndpointingConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.startHistory = reader.int32();
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.startThreshold = reader.float();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.stopHistory = reader.int32();
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.stopThreshold = reader.float();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.stopHistoryEou = reader.int32();
          continue;
        case 6:
          if (tag !== 53) {
            break;
          }

          message.stopThresholdEou = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EndpointingConfig {
    return {
      startHistory: isSet(object.startHistory) ? globalThis.Number(object.startHistory) : undefined,
      startThreshold: isSet(object.startThreshold) ? globalThis.Number(object.startThreshold) : undefined,
      stopHistory: isSet(object.stopHistory) ? globalThis.Number(object.stopHistory) : undefined,
      stopThreshold: isSet(object.stopThreshold) ? globalThis.Number(object.stopThreshold) : undefined,
      stopHistoryEou: isSet(object.stopHistoryEou) ? globalThis.Number(object.stopHistoryEou) : undefined,
      stopThresholdEou: isSet(object.stopThresholdEou) ? globalThis.Number(object.stopThresholdEou) : undefined,
    };
  },

  toJSON(message: EndpointingConfig): unknown {
    const obj: any = {};
    if (message.startHistory !== undefined) {
      obj.startHistory = Math.round(message.startHistory);
    }
    if (message.startThreshold !== undefined) {
      obj.startThreshold = message.startThreshold;
    }
    if (message.stopHistory !== undefined) {
      obj.stopHistory = Math.round(message.stopHistory);
    }
    if (message.stopThreshold !== undefined) {
      obj.stopThreshold = message.stopThreshold;
    }
    if (message.stopHistoryEou !== undefined) {
      obj.stopHistoryEou = Math.round(message.stopHistoryEou);
    }
    if (message.stopThresholdEou !== undefined) {
      obj.stopThresholdEou = message.stopThresholdEou;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<EndpointingConfig>, I>>(base?: I): EndpointingConfig {
    return EndpointingConfig.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<EndpointingConfig>, I>>(object: I): EndpointingConfig {
    const message = createBaseEndpointingConfig();
    message.startHistory = object.startHistory ?? undefined;
    message.startThreshold = object.startThreshold ?? undefined;
    message.stopHistory = object.stopHistory ?? undefined;
    message.stopThreshold = object.stopThreshold ?? undefined;
    message.stopHistoryEou = object.stopHistoryEou ?? undefined;
    message.stopThresholdEou = object.stopThresholdEou ?? undefined;
    return message;
  },
};

function createBaseRecognitionConfig(): RecognitionConfig {
  return {
    encoding: 0,
    sampleRateHertz: 0,
    languageCode: "",
    maxAlternatives: 0,
    profanityFilter: false,
    speechContexts: [],
    audioChannelCount: 0,
    enableWordTimeOffsets: false,
    enableAutomaticPunctuation: false,
    enableSeparateRecognitionPerChannel: false,
    model: "",
    verbatimTranscripts: false,
    diarizationConfig: undefined,
    customConfiguration: {},
    endpointingConfig: undefined,
  };
}

export const RecognitionConfig = {
  encode(message: RecognitionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.encoding !== 0) {
      writer.uint32(8).int32(message.encoding);
    }
    if (message.sampleRateHertz !== 0) {
      writer.uint32(16).int32(message.sampleRateHertz);
    }
    if (message.languageCode !== "") {
      writer.uint32(26).string(message.languageCode);
    }
    if (message.maxAlternatives !== 0) {
      writer.uint32(32).int32(message.maxAlternatives);
    }
    if (message.profanityFilter !== false) {
      writer.uint32(40).bool(message.profanityFilter);
    }
    for (const v of message.speechContexts) {
      SpeechContext.encode(v!, writer.uint32(50).fork()).join();
    }
    if (message.audioChannelCount !== 0) {
      writer.uint32(56).int32(message.audioChannelCount);
    }
    if (message.enableWordTimeOffsets !== false) {
      writer.uint32(64).bool(message.enableWordTimeOffsets);
    }
    if (message.enableAutomaticPunctuation !== false) {
      writer.uint32(88).bool(message.enableAutomaticPunctuation);
    }
    if (message.enableSeparateRecognitionPerChannel !== false) {
      writer.uint32(96).bool(message.enableSeparateRecognitionPerChannel);
    }
    if (message.model !== "") {
      writer.uint32(106).string(message.model);
    }
    if (message.verbatimTranscripts !== false) {
      writer.uint32(112).bool(message.verbatimTranscripts);
    }
    if (message.diarizationConfig !== undefined) {
      SpeakerDiarizationConfig.encode(message.diarizationConfig, writer.uint32(154).fork()).join();
    }
    Object.entries(message.customConfiguration).forEach(([key, value]) => {
      RecognitionConfig_CustomConfigurationEntry.encode({ key: key as any, value }, writer.uint32(194).fork()).join();
    });
    if (message.endpointingConfig !== undefined) {
      EndpointingConfig.encode(message.endpointingConfig, writer.uint32(202).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RecognitionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRecognitionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.encoding = reader.int32() as any;
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.sampleRateHertz = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.languageCode = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.maxAlternatives = reader.int32();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.profanityFilter = reader.bool();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.speechContexts.push(SpeechContext.decode(reader, reader.uint32()));
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.audioChannelCount = reader.int32();
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.enableWordTimeOffsets = reader.bool();
          continue;
        case 11:
          if (tag !== 88) {
            break;
          }

          message.enableAutomaticPunctuation = reader.bool();
          continue;
        case 12:
          if (tag !== 96) {
            break;
          }

          message.enableSeparateRecognitionPerChannel = reader.bool();
          continue;
        case 13:
          if (tag !== 106) {
            break;
          }

          message.model = reader.string();
          continue;
        case 14:
          if (tag !== 112) {
            break;
          }

          message.verbatimTranscripts = reader.bool();
          continue;
        case 19:
          if (tag !== 154) {
            break;
          }

          message.diarizationConfig = SpeakerDiarizationConfig.decode(reader, reader.uint32());
          continue;
        case 24:
          if (tag !== 194) {
            break;
          }

          const entry24 = RecognitionConfig_CustomConfigurationEntry.decode(reader, reader.uint32());
          if (entry24.value !== undefined) {
            message.customConfiguration[entry24.key] = entry24.value;
          }
          continue;
        case 25:
          if (tag !== 202) {
            break;
          }

          message.endpointingConfig = EndpointingConfig.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RecognitionConfig {
    return {
      encoding: isSet(object.encoding) ? audioEncodingFromJSON(object.encoding) : 0,
      sampleRateHertz: isSet(object.sampleRateHertz) ? globalThis.Number(object.sampleRateHertz) : 0,
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
      maxAlternatives: isSet(object.maxAlternatives) ? globalThis.Number(object.maxAlternatives) : 0,
      profanityFilter: isSet(object.profanityFilter) ? globalThis.Boolean(object.profanityFilter) : false,
      speechContexts: globalThis.Array.isArray(object?.speechContexts)
        ? object.speechContexts.map((e: any) => SpeechContext.fromJSON(e))
        : [],
      audioChannelCount: isSet(object.audioChannelCount) ? globalThis.Number(object.audioChannelCount) : 0,
      enableWordTimeOffsets: isSet(object.enableWordTimeOffsets)
        ? globalThis.Boolean(object.enableWordTimeOffsets)
        : false,
      enableAutomaticPunctuation: isSet(object.enableAutomaticPunctuation)
        ? globalThis.Boolean(object.enableAutomaticPunctuation)
        : false,
      enableSeparateRecognitionPerChannel: isSet(object.enableSeparateRecognitionPerChannel)
        ? globalThis.Boolean(object.enableSeparateRecognitionPerChannel)
        : false,
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      verbatimTranscripts: isSet(object.verbatimTranscripts) ? globalThis.Boolean(object.verbatimTranscripts) : false,
      diarizationConfig: isSet(object.diarizationConfig)
        ? SpeakerDiarizationConfig.fromJSON(object.diarizationConfig)
        : undefined,
      customConfiguration: isObject(object.customConfiguration)
        ? Object.entries(object.customConfiguration).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      endpointingConfig: isSet(object.endpointingConfig)
        ? EndpointingConfig.fromJSON(object.endpointingConfig)
        : undefined,
    };
  },

  toJSON(message: RecognitionConfig): unknown {
    const obj: any = {};
    if (message.encoding !== 0) {
      obj.encoding = audioEncodingToJSON(message.encoding);
    }
    if (message.sampleRateHertz !== 0) {
      obj.sampleRateHertz = Math.round(message.sampleRateHertz);
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    if (message.maxAlternatives !== 0) {
      obj.maxAlternatives = Math.round(message.maxAlternatives);
    }
    if (message.profanityFilter !== false) {
      obj.profanityFilter = message.profanityFilter;
    }
    if (message.speechContexts?.length) {
      obj.speechContexts = message.speechContexts.map((e) => SpeechContext.toJSON(e));
    }
    if (message.audioChannelCount !== 0) {
      obj.audioChannelCount = Math.round(message.audioChannelCount);
    }
    if (message.enableWordTimeOffsets !== false) {
      obj.enableWordTimeOffsets = message.enableWordTimeOffsets;
    }
    if (message.enableAutomaticPunctuation !== false) {
      obj.enableAutomaticPunctuation = message.enableAutomaticPunctuation;
    }
    if (message.enableSeparateRecognitionPerChannel !== false) {
      obj.enableSeparateRecognitionPerChannel = message.enableSeparateRecognitionPerChannel;
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.verbatimTranscripts !== false) {
      obj.verbatimTranscripts = message.verbatimTranscripts;
    }
    if (message.diarizationConfig !== undefined) {
      obj.diarizationConfig = SpeakerDiarizationConfig.toJSON(message.diarizationConfig);
    }
    if (message.customConfiguration) {
      const entries = Object.entries(message.customConfiguration);
      if (entries.length > 0) {
        obj.customConfiguration = {};
        entries.forEach(([k, v]) => {
          obj.customConfiguration[k] = v;
        });
      }
    }
    if (message.endpointingConfig !== undefined) {
      obj.endpointingConfig = EndpointingConfig.toJSON(message.endpointingConfig);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<RecognitionConfig>, I>>(base?: I): RecognitionConfig {
    return RecognitionConfig.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<RecognitionConfig>, I>>(object: I): RecognitionConfig {
    const message = createBaseRecognitionConfig();
    message.encoding = object.encoding ?? 0;
    message.sampleRateHertz = object.sampleRateHertz ?? 0;
    message.languageCode = object.languageCode ?? "";
    message.maxAlternatives = object.maxAlternatives ?? 0;
    message.profanityFilter = object.profanityFilter ?? false;
    message.speechContexts = object.speechContexts?.map((e) => SpeechContext.fromPartial(e)) || [];
    message.audioChannelCount = object.audioChannelCount ?? 0;
    message.enableWordTimeOffsets = object.enableWordTimeOffsets ?? false;
    message.enableAutomaticPunctuation = object.enableAutomaticPunctuation ?? false;
    message.enableSeparateRecognitionPerChannel = object.enableSeparateRecognitionPerChannel ?? false;
    message.model = object.model ?? "";
    message.verbatimTranscripts = object.verbatimTranscripts ?? false;
    message.diarizationConfig = (object.diarizationConfig !== undefined && object.diarizationConfig !== null)
      ? SpeakerDiarizationConfig.fromPartial(object.diarizationConfig)
      : undefined;
    message.customConfiguration = Object.entries(object.customConfiguration ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.endpointingConfig = (object.endpointingConfig !== undefined && object.endpointingConfig !== null)
      ? EndpointingConfig.fromPartial(object.endpointingConfig)
      : undefined;
    return message;
  },
};

function createBaseRecognitionConfig_CustomConfigurationEntry(): RecognitionConfig_CustomConfigurationEntry {
  return { key: "", value: "" };
}

export const RecognitionConfig_CustomConfigurationEntry = {
  encode(message: RecognitionConfig_CustomConfigurationEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RecognitionConfig_CustomConfigurationEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRecognitionConfig_CustomConfigurationEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RecognitionConfig_CustomConfigurationEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: RecognitionConfig_CustomConfigurationEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<RecognitionConfig_CustomConfigurationEntry>, I>>(
    base?: I,
  ): RecognitionConfig_CustomConfigurationEntry {
    return RecognitionConfig_CustomConfigurationEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<RecognitionConfig_CustomConfigurationEntry>, I>>(
    object: I,
  ): RecognitionConfig_CustomConfigurationEntry {
    const message = createBaseRecognitionConfig_CustomConfigurationEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseStreamingRecognitionConfig(): StreamingRecognitionConfig {
  return { config: undefined, interimResults: false };
}

export const StreamingRecognitionConfig = {
  encode(message: StreamingRecognitionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.config !== undefined) {
      RecognitionConfig.encode(message.config, writer.uint32(10).fork()).join();
    }
    if (message.interimResults !== false) {
      writer.uint32(16).bool(message.interimResults);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingRecognitionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingRecognitionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.config = RecognitionConfig.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.interimResults = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingRecognitionConfig {
    return {
      config: isSet(object.config) ? RecognitionConfig.fromJSON(object.config) : undefined,
      interimResults: isSet(object.interimResults) ? globalThis.Boolean(object.interimResults) : false,
    };
  },

  toJSON(message: StreamingRecognitionConfig): unknown {
    const obj: any = {};
    if (message.config !== undefined) {
      obj.config = RecognitionConfig.toJSON(message.config);
    }
    if (message.interimResults !== false) {
      obj.interimResults = message.interimResults;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<StreamingRecognitionConfig>, I>>(base?: I): StreamingRecognitionConfig {
    return StreamingRecognitionConfig.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<StreamingRecognitionConfig>, I>>(object: I): StreamingRecognitionConfig {
    const message = createBaseStreamingRecognitionConfig();
    message.config = (object.config !== undefined && object.config !== null)
      ? RecognitionConfig.fromPartial(object.config)
      : undefined;
    message.interimResults = object.interimResults ?? false;
    return message;
  },
};

function createBaseSpeakerDiarizationConfig(): SpeakerDiarizationConfig {
  return { enableSpeakerDiarization: false, maxSpeakerCount: 0 };
}

export const SpeakerDiarizationConfig = {
  encode(message: SpeakerDiarizationConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.enableSpeakerDiarization !== false) {
      writer.uint32(8).bool(message.enableSpeakerDiarization);
    }
    if (message.maxSpeakerCount !== 0) {
      writer.uint32(16).int32(message.maxSpeakerCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeakerDiarizationConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeakerDiarizationConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.enableSpeakerDiarization = reader.bool();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.maxSpeakerCount = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeakerDiarizationConfig {
    return {
      enableSpeakerDiarization: isSet(object.enableSpeakerDiarization)
        ? globalThis.Boolean(object.enableSpeakerDiarization)
        : false,
      maxSpeakerCount: isSet(object.maxSpeakerCount) ? globalThis.Number(object.maxSpeakerCount) : 0,
    };
  },

  toJSON(message: SpeakerDiarizationConfig): unknown {
    const obj: any = {};
    if (message.enableSpeakerDiarization !== false) {
      obj.enableSpeakerDiarization = message.enableSpeakerDiarization;
    }
    if (message.maxSpeakerCount !== 0) {
      obj.maxSpeakerCount = Math.round(message.maxSpeakerCount);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SpeakerDiarizationConfig>, I>>(base?: I): SpeakerDiarizationConfig {
    return SpeakerDiarizationConfig.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SpeakerDiarizationConfig>, I>>(object: I): SpeakerDiarizationConfig {
    const message = createBaseSpeakerDiarizationConfig();
    message.enableSpeakerDiarization = object.enableSpeakerDiarization ?? false;
    message.maxSpeakerCount = object.maxSpeakerCount ?? 0;
    return message;
  },
};

function createBaseSpeechContext(): SpeechContext {
  return { phrases: [], boost: 0 };
}

export const SpeechContext = {
  encode(message: SpeechContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.phrases) {
      writer.uint32(10).string(v!);
    }
    if (message.boost !== 0) {
      writer.uint32(37).float(message.boost);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.phrases.push(reader.string());
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.boost = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechContext {
    return {
      phrases: globalThis.Array.isArray(object?.phrases) ? object.phrases.map((e: any) => globalThis.String(e)) : [],
      boost: isSet(object.boost) ? globalThis.Number(object.boost) : 0,
    };
  },

  toJSON(message: SpeechContext): unknown {
    const obj: any = {};
    if (message.phrases?.length) {
      obj.phrases = message.phrases;
    }
    if (message.boost !== 0) {
      obj.boost = message.boost;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SpeechContext>, I>>(base?: I): SpeechContext {
    return SpeechContext.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SpeechContext>, I>>(object: I): SpeechContext {
    const message = createBaseSpeechContext();
    message.phrases = object.phrases?.map((e) => e) || [];
    message.boost = object.boost ?? 0;
    return message;
  },
};

function createBaseRecognizeResponse(): RecognizeResponse {
  return { results: [], id: undefined };
}

export const RecognizeResponse = {
  encode(message: RecognizeResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.results) {
      SpeechRecognitionResult.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.id !== undefined) {
      RequestId.encode(message.id, writer.uint32(802).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RecognizeResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRecognizeResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.results.push(SpeechRecognitionResult.decode(reader, reader.uint32()));
          continue;
        case 100:
          if (tag !== 802) {
            break;
          }

          message.id = RequestId.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RecognizeResponse {
    return {
      results: globalThis.Array.isArray(object?.results)
        ? object.results.map((e: any) => SpeechRecognitionResult.fromJSON(e))
        : [],
      id: isSet(object.id) ? RequestId.fromJSON(object.id) : undefined,
    };
  },

  toJSON(message: RecognizeResponse): unknown {
    const obj: any = {};
    if (message.results?.length) {
      obj.results = message.results.map((e) => SpeechRecognitionResult.toJSON(e));
    }
    if (message.id !== undefined) {
      obj.id = RequestId.toJSON(message.id);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<RecognizeResponse>, I>>(base?: I): RecognizeResponse {
    return RecognizeResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<RecognizeResponse>, I>>(object: I): RecognizeResponse {
    const message = createBaseRecognizeResponse();
    message.results = object.results?.map((e) => SpeechRecognitionResult.fromPartial(e)) || [];
    message.id = (object.id !== undefined && object.id !== null) ? RequestId.fromPartial(object.id) : undefined;
    return message;
  },
};

function createBaseSpeechRecognitionResult(): SpeechRecognitionResult {
  return { alternatives: [], channelTag: 0, audioProcessed: 0 };
}

export const SpeechRecognitionResult = {
  encode(message: SpeechRecognitionResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.alternatives) {
      SpeechRecognitionAlternative.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.channelTag !== 0) {
      writer.uint32(16).int32(message.channelTag);
    }
    if (message.audioProcessed !== 0) {
      writer.uint32(29).float(message.audioProcessed);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechRecognitionResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechRecognitionResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.alternatives.push(SpeechRecognitionAlternative.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.channelTag = reader.int32();
          continue;
        case 3:
          if (tag !== 29) {
            break;
          }

          message.audioProcessed = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechRecognitionResult {
    return {
      alternatives: globalThis.Array.isArray(object?.alternatives)
        ? object.alternatives.map((e: any) => SpeechRecognitionAlternative.fromJSON(e))
        : [],
      channelTag: isSet(object.channelTag) ? globalThis.Number(object.channelTag) : 0,
      audioProcessed: isSet(object.audioProcessed) ? globalThis.Number(object.audioProcessed) : 0,
    };
  },

  toJSON(message: SpeechRecognitionResult): unknown {
    const obj: any = {};
    if (message.alternatives?.length) {
      obj.alternatives = message.alternatives.map((e) => SpeechRecognitionAlternative.toJSON(e));
    }
    if (message.channelTag !== 0) {
      obj.channelTag = Math.round(message.channelTag);
    }
    if (message.audioProcessed !== 0) {
      obj.audioProcessed = message.audioProcessed;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SpeechRecognitionResult>, I>>(base?: I): SpeechRecognitionResult {
    return SpeechRecognitionResult.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SpeechRecognitionResult>, I>>(object: I): SpeechRecognitionResult {
    const message = createBaseSpeechRecognitionResult();
    message.alternatives = object.alternatives?.map((e) => SpeechRecognitionAlternative.fromPartial(e)) || [];
    message.channelTag = object.channelTag ?? 0;
    message.audioProcessed = object.audioProcessed ?? 0;
    return message;
  },
};

function createBaseSpeechRecognitionAlternative(): SpeechRecognitionAlternative {
  return { transcript: "", confidence: 0, words: [] };
}

export const SpeechRecognitionAlternative = {
  encode(message: SpeechRecognitionAlternative, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.transcript !== "") {
      writer.uint32(10).string(message.transcript);
    }
    if (message.confidence !== 0) {
      writer.uint32(21).float(message.confidence);
    }
    for (const v of message.words) {
      WordInfo.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechRecognitionAlternative {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechRecognitionAlternative();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.transcript = reader.string();
          continue;
        case 2:
          if (tag !== 21) {
            break;
          }

          message.confidence = reader.float();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.words.push(WordInfo.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechRecognitionAlternative {
    return {
      transcript: isSet(object.transcript) ? globalThis.String(object.transcript) : "",
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      words: globalThis.Array.isArray(object?.words) ? object.words.map((e: any) => WordInfo.fromJSON(e)) : [],
    };
  },

  toJSON(message: SpeechRecognitionAlternative): unknown {
    const obj: any = {};
    if (message.transcript !== "") {
      obj.transcript = message.transcript;
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.words?.length) {
      obj.words = message.words.map((e) => WordInfo.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SpeechRecognitionAlternative>, I>>(base?: I): SpeechRecognitionAlternative {
    return SpeechRecognitionAlternative.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SpeechRecognitionAlternative>, I>>(object: I): SpeechRecognitionAlternative {
    const message = createBaseSpeechRecognitionAlternative();
    message.transcript = object.transcript ?? "";
    message.confidence = object.confidence ?? 0;
    message.words = object.words?.map((e) => WordInfo.fromPartial(e)) || [];
    return message;
  },
};

function createBaseWordInfo(): WordInfo {
  return { startTime: 0, endTime: 0, word: "", confidence: 0, speakerTag: 0 };
}

export const WordInfo = {
  encode(message: WordInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startTime !== 0) {
      writer.uint32(8).int32(message.startTime);
    }
    if (message.endTime !== 0) {
      writer.uint32(16).int32(message.endTime);
    }
    if (message.word !== "") {
      writer.uint32(26).string(message.word);
    }
    if (message.confidence !== 0) {
      writer.uint32(37).float(message.confidence);
    }
    if (message.speakerTag !== 0) {
      writer.uint32(40).int32(message.speakerTag);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): WordInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseWordInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.startTime = reader.int32();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.endTime = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.word = reader.string();
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.confidence = reader.float();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.speakerTag = reader.int32();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): WordInfo {
    return {
      startTime: isSet(object.startTime) ? globalThis.Number(object.startTime) : 0,
      endTime: isSet(object.endTime) ? globalThis.Number(object.endTime) : 0,
      word: isSet(object.word) ? globalThis.String(object.word) : "",
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      speakerTag: isSet(object.speakerTag) ? globalThis.Number(object.speakerTag) : 0,
    };
  },

  toJSON(message: WordInfo): unknown {
    const obj: any = {};
    if (message.startTime !== 0) {
      obj.startTime = Math.round(message.startTime);
    }
    if (message.endTime !== 0) {
      obj.endTime = Math.round(message.endTime);
    }
    if (message.word !== "") {
      obj.word = message.word;
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.speakerTag !== 0) {
      obj.speakerTag = Math.round(message.speakerTag);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<WordInfo>, I>>(base?: I): WordInfo {
    return WordInfo.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<WordInfo>, I>>(object: I): WordInfo {
    const message = createBaseWordInfo();
    message.startTime = object.startTime ?? 0;
    message.endTime = object.endTime ?? 0;
    message.word = object.word ?? "";
    message.confidence = object.confidence ?? 0;
    message.speakerTag = object.speakerTag ?? 0;
    return message;
  },
};

function createBaseStreamingRecognizeResponse(): StreamingRecognizeResponse {
  return { results: [], id: undefined };
}

export const StreamingRecognizeResponse = {
  encode(message: StreamingRecognizeResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.results) {
      StreamingRecognitionResult.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.id !== undefined) {
      RequestId.encode(message.id, writer.uint32(802).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingRecognizeResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingRecognizeResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.results.push(StreamingRecognitionResult.decode(reader, reader.uint32()));
          continue;
        case 100:
          if (tag !== 802) {
            break;
          }

          message.id = RequestId.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingRecognizeResponse {
    return {
      results: globalThis.Array.isArray(object?.results)
        ? object.results.map((e: any) => StreamingRecognitionResult.fromJSON(e))
        : [],
      id: isSet(object.id) ? RequestId.fromJSON(object.id) : undefined,
    };
  },

  toJSON(message: StreamingRecognizeResponse): unknown {
    const obj: any = {};
    if (message.results?.length) {
      obj.results = message.results.map((e) => StreamingRecognitionResult.toJSON(e));
    }
    if (message.id !== undefined) {
      obj.id = RequestId.toJSON(message.id);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<StreamingRecognizeResponse>, I>>(base?: I): StreamingRecognizeResponse {
    return StreamingRecognizeResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<StreamingRecognizeResponse>, I>>(object: I): StreamingRecognizeResponse {
    const message = createBaseStreamingRecognizeResponse();
    message.results = object.results?.map((e) => StreamingRecognitionResult.fromPartial(e)) || [];
    message.id = (object.id !== undefined && object.id !== null) ? RequestId.fromPartial(object.id) : undefined;
    return message;
  },
};

function createBaseStreamingRecognitionResult(): StreamingRecognitionResult {
  return { alternatives: [], isFinal: false, stability: 0, channelTag: 0, audioProcessed: 0 };
}

export const StreamingRecognitionResult = {
  encode(message: StreamingRecognitionResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.alternatives) {
      SpeechRecognitionAlternative.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.isFinal !== false) {
      writer.uint32(16).bool(message.isFinal);
    }
    if (message.stability !== 0) {
      writer.uint32(29).float(message.stability);
    }
    if (message.channelTag !== 0) {
      writer.uint32(40).int32(message.channelTag);
    }
    if (message.audioProcessed !== 0) {
      writer.uint32(53).float(message.audioProcessed);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingRecognitionResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingRecognitionResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.alternatives.push(SpeechRecognitionAlternative.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.isFinal = reader.bool();
          continue;
        case 3:
          if (tag !== 29) {
            break;
          }

          message.stability = reader.float();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.channelTag = reader.int32();
          continue;
        case 6:
          if (tag !== 53) {
            break;
          }

          message.audioProcessed = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingRecognitionResult {
    return {
      alternatives: globalThis.Array.isArray(object?.alternatives)
        ? object.alternatives.map((e: any) => SpeechRecognitionAlternative.fromJSON(e))
        : [],
      isFinal: isSet(object.isFinal) ? globalThis.Boolean(object.isFinal) : false,
      stability: isSet(object.stability) ? globalThis.Number(object.stability) : 0,
      channelTag: isSet(object.channelTag) ? globalThis.Number(object.channelTag) : 0,
      audioProcessed: isSet(object.audioProcessed) ? globalThis.Number(object.audioProcessed) : 0,
    };
  },

  toJSON(message: StreamingRecognitionResult): unknown {
    const obj: any = {};
    if (message.alternatives?.length) {
      obj.alternatives = message.alternatives.map((e) => SpeechRecognitionAlternative.toJSON(e));
    }
    if (message.isFinal !== false) {
      obj.isFinal = message.isFinal;
    }
    if (message.stability !== 0) {
      obj.stability = message.stability;
    }
    if (message.channelTag !== 0) {
      obj.channelTag = Math.round(message.channelTag);
    }
    if (message.audioProcessed !== 0) {
      obj.audioProcessed = message.audioProcessed;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<StreamingRecognitionResult>, I>>(base?: I): StreamingRecognitionResult {
    return StreamingRecognitionResult.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<StreamingRecognitionResult>, I>>(object: I): StreamingRecognitionResult {
    const message = createBaseStreamingRecognitionResult();
    message.alternatives = object.alternatives?.map((e) => SpeechRecognitionAlternative.fromPartial(e)) || [];
    message.isFinal = object.isFinal ?? false;
    message.stability = object.stability ?? 0;
    message.channelTag = object.channelTag ?? 0;
    message.audioProcessed = object.audioProcessed ?? 0;
    return message;
  },
};

/**
 * The RivaSpeechRecognition service provides two mechanisms for converting
 * speech to text.
 */
export type RivaSpeechRecognitionService = typeof RivaSpeechRecognitionService;
export const RivaSpeechRecognitionService = {
  /**
   * Recognize expects a RecognizeRequest and returns a RecognizeResponse. This
   * request will block until the audio is uploaded, processed, and a transcript
   * is returned.
   */
  recognize: {
    path: "/nvidia.riva.asr.RivaSpeechRecognition/Recognize",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: RecognizeRequest) => Buffer.from(RecognizeRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => RecognizeRequest.decode(value),
    responseSerialize: (value: RecognizeResponse) => Buffer.from(RecognizeResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => RecognizeResponse.decode(value),
  },
  /**
   * StreamingRecognize is a non-blocking API call that allows audio data to be
   * fed to the server in chunks as it becomes available. Depending on the
   * configuration in the StreamingRecognizeRequest, intermediate results can be
   * sent back to the client. Recognition ends when the stream is closed by the
   * client.
   */
  streamingRecognize: {
    path: "/nvidia.riva.asr.RivaSpeechRecognition/StreamingRecognize",
    requestStream: true,
    responseStream: true,
    requestSerialize: (value: StreamingRecognizeRequest) =>
      Buffer.from(StreamingRecognizeRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => StreamingRecognizeRequest.decode(value),
    responseSerialize: (value: StreamingRecognizeResponse) =>
      Buffer.from(StreamingRecognizeResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => StreamingRecognizeResponse.decode(value),
  },
  /**
   * Enables clients to request the configuration of the current ASR service, or
   * a specific model within the service.
   */
  getRivaSpeechRecognitionConfig: {
    path: "/nvidia.riva.asr.RivaSpeechRecognition/GetRivaSpeechRecognitionConfig",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: RivaSpeechRecognitionConfigRequest) =>
      Buffer.from(RivaSpeechRecognitionConfigRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => RivaSpeechRecognitionConfigRequest.decode(value),
    responseSerialize: (value: RivaSpeechRecognitionConfigResponse) =>
      Buffer.from(RivaSpeechRecognitionConfigResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => RivaSpeechRecognitionConfigResponse.decode(value),
  },
} as const;

export interface RivaSpeechRecognitionServer extends UntypedServiceImplementation {
  /**
   * Recognize expects a RecognizeRequest and returns a RecognizeResponse. This
   * request will block until the audio is uploaded, processed, and a transcript
   * is returned.
   */
  recognize: handleUnaryCall<RecognizeRequest, RecognizeResponse>;
  /**
   * StreamingRecognize is a non-blocking API call that allows audio data to be
   * fed to the server in chunks as it becomes available. Depending on the
   * configuration in the StreamingRecognizeRequest, intermediate results can be
   * sent back to the client. Recognition ends when the stream is closed by the
   * client.
   */
  streamingRecognize: handleBidiStreamingCall<StreamingRecognizeRequest, StreamingRecognizeResponse>;
  /**
   * Enables clients to request the configuration of the current ASR service, or
   * a specific model within the service.
   */
  getRivaSpeechRecognitionConfig: handleUnaryCall<
    RivaSpeechRecognitionConfigRequest,
    RivaSpeechRecognitionConfigResponse
  >;
}

export interface RivaSpeechRecognitionClient extends Client {
  /**
   * Recognize expects a RecognizeRequest and returns a RecognizeResponse. This
   * request will block until the audio is uploaded, processed, and a transcript
   * is returned.
   */
  recognize(
    request: RecognizeRequest,
    callback: (error: ServiceError | null, response: RecognizeResponse) => void,
  ): ClientUnaryCall;
  recognize(
    request: RecognizeRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: RecognizeResponse) => void,
  ): ClientUnaryCall;
  recognize(
    request: RecognizeRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: RecognizeResponse) => void,
  ): ClientUnaryCall;
  /**
   * StreamingRecognize is a non-blocking API call that allows audio data to be
   * fed to the server in chunks as it becomes available. Depending on the
   * configuration in the StreamingRecognizeRequest, intermediate results can be
   * sent back to the client. Recognition ends when the stream is closed by the
   * client.
   */
  streamingRecognize(): ClientDuplexStream<StreamingRecognizeRequest, StreamingRecognizeResponse>;
  streamingRecognize(
    options: Partial<CallOptions>,
  ): ClientDuplexStream<StreamingRecognizeRequest, StreamingRecognizeResponse>;
  streamingRecognize(
    metadata: Metadata,
    options?: Partial<CallOptions>,
  ): ClientDuplexStream<StreamingRecognizeRequest, StreamingRecognizeResponse>;
  /**
   * Enables clients to request the configuration of the current ASR service, or
   * a specific model within the service.
   */
  getRivaSpeechRecognitionConfig(
    request: RivaSpeechRecognitionConfigRequest,
    callback: (error: ServiceError | null, response: RivaSpeechRecognitionConfigResponse) => void,
  ): ClientUnaryCall;
  getRivaSpeechRecognitionConfig(
    request: RivaSpeechRecognitionConfigRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: RivaSpeechRecognitionConfigResponse) => void,
  ): ClientUnaryCall;
  getRivaSpeechRecognitionConfig(
    request: RivaSpeechRecognitionConfigRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: RivaSpeechRecognitionConfigResponse) => void,
  ): ClientUnaryCall;
}

export const RivaSpeechRecognitionClient = makeGenericClientConstructor(
  RivaSpeechRecognitionService,
  "nvidia.riva.asr.RivaSpeechRecognition",
) as unknown as {
  new (address: string, credentials: ChannelCredentials, options?: Partial<ClientOptions>): RivaSpeechRecognitionClient;
  service: typeof RivaSpeechRecognitionService;
  serviceName: string;
};

function bytesFromBase64(b64: string): Uint8Array {
  if ((globalThis as any).Buffer) {
    return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
  } else {
    const bin = globalThis.atob(b64);
    const arr = new Uint8Array(bin.length);
    for (let i = 0; i < bin.length; ++i) {
      arr[i] = bin.charCodeAt(i);
    }
    return arr;
  }
}

function base64FromBytes(arr: Uint8Array): string {
  if ((globalThis as any).Buffer) {
    return globalThis.Buffer.from(arr).toString("base64");
  } else {
    const bin: string[] = [];
    arr.forEach((byte) => {
      bin.push(globalThis.String.fromCharCode(byte));
    });
    return globalThis.btoa(bin.join(""));
  }
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

type KeysOfUnion<T> = T extends T ? keyof T : never;
export type Exact<P, I extends P> = P extends Builtin ? P
  : P & { [K in keyof P]: Exact<P[K], I[K]> } & { [K in Exclude<keyof I, KeysOfUnion<P>>]: never };

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}
